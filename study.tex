%!TEX root = bug-taxo.tex

\section{Study Design}

The goal of this study is to analyze the location of bug fixes, with the purpose of classifying bug fixes into types.
More specifically, this study aims to answer the folliwing two research questions:

\begin{itemize}
	\item {\bf RQ$_1$:} {\it What are the proportions of different types of bugs?} This research question aims to what extent bug can be classified according to their fix-locations and the proportion of each types.
  Specifically, we investigate if different types of bugs exists at all and if the proportion of different types in non-negligible.
  As discussed ealier, knowing, for example, that bugs of Type 2 and 4 are the most predominant ones suggests that we need to investigate techniques to help detect whether an incoming bug is of Types 2 and 4 by examining historical data.
  Similarly, if we can automatically identify a bug that is related to another one that has been fixed then we can reuse the results of reproducing the first bug in reproducing the second one.

	\item {\bf RQ$_2$:} {\it How complex is each type of bugs?} This second research question aims to investigate the complexity of the different types of bug.
  More specifically, we analyze and discuss the complexity of different types of bugs using code and process metrics both.
  For the code aspect of the complexity, we compute the number of different files impacted by the fix and the number of hunks and churns.
  We do not compute any statical complexity metrics such as cyclomatic complexity \cite{McCabe1989}.
  For the process aspect of complexity, we analyze the severity of the bug, the amount of duplicate bug report submitted, the number of times a bug report gets reopened, the number of comments and the time required to fix the bug.

\end{itemize}

\subsection{Context Selection}

The context of this study consists of the change history of 388 projects belonging to two software ecosystem, namely, Apache and Netbeans.
Tbale \ref{table:datasets} reports, for each of them, (i) the number of projects analyzed, (ii) size ranges in terms of the number of classes and KLOC, (iii) the overall number of commits and issues analyzed, and (iv) the average, minimum, and maximum length of the projects' story (in years).

\begin{table}[h]
\begin{center}
\begin{tabular}{@{}c|c|c|c|c@{}}
\textbf{Dataset} & \textbf{R/F BR} & \textbf{CS} & \textbf{Files} & \textbf{Projects} \\ \hline \hline
Netbeans         & 53,258          & 122,632     & 30,595         & 39                \\
Apache           & 49,449          & 106,366     & 38,111         & 349               \\
Total            & 102,707         & 229,153     & 68,809         & 388               \\ \hline \hline

\end{tabular}
\end{center}

\caption{Datasets\label{table:datasets}}
\end{table}



All the analysed projects are hosted in {\it Git} or {\it Mercurial} repositories and have either a {\it Jira} or a {\it Bugzilla} issue tracker associated with it.
The Apache ecosystem consists in 349 projects written in various programming languages (C, C++, Java, Python, ...) and uses {\it Git} and {\it Jira}.
These projects represent the Apache ecosystem in its entirety; no system has been excluded from our study.
The complete list can be found online\footnote{https://projects.apache.org/projects.html?name}.
The Netbeans ecosystem consists in 39 projects mostly written in Java.
Similarly to the Apache ecosystem, we did not select some of the projects belonging to the Netbeans ecosystem but all of them.
The Netbeans community uses {\it Bugzilla} and {\it Mercurial}.

The choice of the ecosystems to analyze is not random, but rather driven by the motivation to consider projects having (i) different sizes, (ii) different architectures, and (iii) different development bases and processes.
Indeed, Apache project are extremely various in terms of size of the development team, purpose and technical choices \cite{Bavota2013}.
On the other side, Netbeans have a relatively stable list of core developer and a common vision shared through the 39 related projects \cite{Wang2011}.

Cumulatively, these datasets span from 2001 to 2014. In summary, our consolidated dataset contains 102,707 bugs, 229,153 changesets, 68,809 files that have been modified to fix the bugs, 462,848 comments, and 388 distinct systems.
We also collected 221 million lines of code modified to fix the bugs, identified 3,284 sub-projects, and 17,984 unique contributors to these bug report and source code version management systems.
The cumulated opening time for all the bugs reaches 10,661 working years (3,891,618 working days) which translates into more than one billion dollars\cite{Usnews}.

\subsection{Data Extraction and Analysis}

This subsection describes the data extraction and analysis process that we followed to answer our research questions.

\subsubsection{What are the proportions of different types of bugs?} To answer {\bf RQ$_1$}, we cloned the 349 {\it git} repositories belonging to the Apache ecosystem and the 39 {\it mercurial} repositories belonging to the Netbeans ecosystem.
The raw size of the cloned source code alone, exluding binaries, images and other non-text file, is 163 GB.
Then, we extracted all the 102,707 closed issue that have been resolved using the {\it RESOLVED/FIXED} tags.
Indeed, this study aims to classify bugs according to their fix locations.
If an issue is fixed by other means than {\it fixing} the source code, then, it falls outside the scope our study.
In order to assign commits to issues we used is the regular expression-based approach by Fischer et al. \cite{Fischer} matching the issue ID in the commit note.
Using this technic, we were able to link almost 40\% (40,493 out of 102,707) of our resolved/fixed issues to 229,153 commits.
An issue can be fixed with several commits.

We choose not to use more complex technics like ReLink, an approach proposed by Wu et al.\cite{Wu2011}, which considers the following constraints: (i) matching the committer/authors with issue tracking contributor name/email; (ii) the time interval between the commit and the last comment posted by the same author/contributor on the issue tracker must be less than seven days; and (iii) Vector Space Model (VSM) cosine similarity between the commit note and the last comment referred above or greater than 0.7 because we belive that mining more than forty thousands issues is enough to be significant.

Using our generated consolidated dataset, we extracted the files $f_i$ impacted by each commit $c_i$ for each one of our 388 projects.
Then, we classify the bugs according to the following:

\begin{itemize}
  \item {\bf Type 1:} A bug is tagged type 1 if it is fixed by modifying a file $f_i$ and $f_i$ is not involved in any other bug fix.
  \item {\bf Type 2:} A bug is tagged type 2 if it is fixed by modifying several files $f_{i..n}$ and the files $f_{i..n}$ are not involved in any other bug fix.
  \item {\bf Type 3:} A bug is tagged type 3 if it is fixed by modifying a file $f_{i}$ and the file $f_{i}$ is involved in fixing other bugs.
  \item {\bf Type 4:} A bug is tagged type 4 if it is fixed by modifying several files $f_{i..n}$ and the files $f_{i..n}$ are involved in any other bug fix.
\end{itemize}

To answer this question, we analyze whether any type is predominant in the studied ecosystem, by testing the null hypothesis:

\begin{itemize}
	\item $H_{01A}$ : The proportion of Types does not
change significantly across the studied ecosystems
\end{itemize}

We test this hypothesis by observing both a ``global'' (across ecosystem) and a ``local'' predominance (per ecosystem) of the different types of bugs.
We must observe these two aspects to ensure that the predominance of a particular type of bug is not circumstantial (in few given systems only) but is also not due to some other, unknown factors (in all systems but not in a particular ecosystem).

We answer {\bf RQ$_1$} in two steps.
The first step is to use descriptive statistics; we compute the ratio of each types to the total number of bugs in the dataset.

In the second step, we compare the proportions of the different types of bugs with respect to the ecosystem where the bugs were found.
We build the contingency table with these two qualitative variables (the type and studied ecosystem) and test the null hypothesis {\bf H$_{01A}$} to assess whether the proportion of a particular type of bugs is related to a specific ecosystem or not.

We use the Pearson's chi-squared test to reject the null hypothesis $H_{01A}$.
Pearson's chi-squared independence test is used to analyze the relationship between two qualitative data, in our study the type bugs and the studied ecosystem.
The results of Pearson's chi-squared independence test are considered
statistically significant at $\alpha$ = 0.05.
If p-value $\le$ 0.05, we reject the null hypothesis $H_{01A}$ and conclude that the proportion of each types is different for each ecosystem.

Overall, the data extraction and manipulation for {\bf RQ$_1$} (i.e., cloning repositories, linking commits to issues and tagging issues by type) took thirteen weeks on two Linux servers having 1 quadcore
3.10 GHz CPU and 12 Gb of RAM each.

\subsubsection{How complex is each type of bugs?}  To answer {\bf RQ$_2$} we went through our 40,493 of our resolved/fixed issues and the linked 229,153 commits in order to compute code and process metrics for each of them.
These metrics will then be used to assess the complexity of a bug.
The computed process metrics are:

\begin{itemize}
  \item The time $t$ it took to resolve issue $i$.
  \item The number of issues $dup$ tagged as duplicate of issue $i$.
  \item The number of time issue $i$ got reopen $reop$.
  \item The number of comments $comment$ on issue $i$.
  \item The severity $sev$ of the issue $i$.
\end{itemize}

The computed code metrics are:

\begin{itemize}
  \item The number of files $f$ impacted by issue $i$.
  \item The number of commit $c$ required to fix the issue $i$.
  \item The number of hunks $h$ required to fix the issue $i$.
  \item The number of churns $ch$ required to fix the issue $i$.
\end{itemize}


We address the relation between types and the complexity of the bugs in using our metrics.
We analyze whether Types 2 and 4 bugs are more complex to handle than Types 1 and 3 bugs, by testing the null hypotheses:

\begin{itemize}
 \item  $H_{02S}$ : The severity of different types is not significantly different
 \item  $H_{02D}$ : Different types are not significantly more likely to get duplicated.
 \item  $H_{02R}$ : Different types are not significantly more likely to get reopened.
 \item $H_{02T}$ : There is no statistically-significant difference
between the duration of fixing of different types.
\end{itemize}

For each hypothesis, we build a contingency table with the qualitative variables type of bugs and the dependent variable.

We use the Pearson's chi-squared test to reject the null
hypothesis $H_{02D}$ (respectively $H_{02R}$ ) and $H_{02S}$. The results of Pearson's chi-squared independence test are considered
statistically significant at $\alpha$ = 0.05.
If a p-value $\le$ 0.05, we reject the null hypothesis $H_{02D}$ (respectively $H_{02R}$) andconclude the fact that the bug is more likely to be duplicated (respectively reopened) is related to the type of the bug and we reject $H_{02S}$ and conclude that the severity level of the bug is related to the bug type.

\section{Analysis of the Results}



This section reports the analysis of the results achieved
aiming at answering our two research questions.

\subsection{What are the proportions of different types of bugs?}

\input{tabs/bugProportion}

Table \ref{tab:contingency-table} presents a contingency table and the results of the Pearson's chi-squared tests we performed on each types of bug.
In addition to presenting bug types 1 to 4,  Table \ref{tab:contingency-table} also presents regroupement of bug types:
(a) Types 1 and 2 versus Types 3 and 4 and (b) Types 1 and 3 versus Types 2 and 4.


Types 3 (22.6\% and 54\%) and 4 (31.3\% and 64.9\%) are predominants compared to types 1 (14.3\% and 9.1\%) and 2 (6.8\% and 3.7\%) for the Apache and the Netbeans ecosystems, respectively.
Obviously, this observation also holds for when the two ecosystems are combined.
Overall, the proportion of different types of bug is as follows: 6.8\%, 3.7\%, 28.3\%, 61.2\% for types 1, 2, 3 and 4, respectively.
The result of the Pearson's tests are bellow 0.01.
As a reminder, we consider results of Pearson's tests statistically significant at $\alpha \textless0.05$.
Consequently, we can conclude that there is a predominance of Types 3 and 4 in all different ecosystems and this observation is not related to a specific ecosystem.
When combined into our first group, Types 1 \& 2 versus. Types 3 \& 4, there are significantly more Types 3 and 4 (89.5 \%) than Types 1 and 2 (10.5 \%).
In the second group, Types 1 \& 3 versus. Types 2 \& 4, there are significantly more Types 2 \& and 4 (64.9\%) than Types 1 \& 3 (35.1\%).



\subsection{How complex is each type of bugs?}

To answer {\bf RQ$_2$}, we analyse the complexity of each bug in terms of duplication, fixing time, comments, reopenning, files impacted, severity, changesets, hunks and chunks.

Figure \ref{fig:boxplots} presents nine boxplots describing our complexity metric for each types of each ecosystem.
In each sub-figure, the boxplots are organised as follows: (a) Types 1 to 4 of the Apache ecosystem, (b) Types 1 to 4 of the Netbeans ecosystem and (c) Types 1 to 4 of both ecosystems combined.
For all the metrics, except the severity, the median is close to zero and we can observe many outliers.
Tables \ref{tab:apache-eco}, \ref{tab:netbeans-eco} and \ref{tab:overall-eco} present descriptive statistics about each metric for each type for the Apache ecosystem, the Netbeans ecosystem and both ecosystems combined, respectively.
The descriptive Statistics used are $\mu$:mean, $\sum$:sum, $\hat{x}$:median, $\sigma$:standard deviation and $\%$:percentage.
In addition, to the descriptive statistics, these tables present matrixes of Mann-Whitney test for each metric and type.
We added the \checkmark~symbol to the Mann-Whitney test results columns when the value is statistically significant (e.g. $\alpha \textless 0.05$) and \xmark~otherwise.

Finally, Table \ref{tab:chi-rq2} presents the Pearson's chi-squared tests results for each complexity metrics for Types 1 to 4 and our two types combination.
In what follows, we present our findings for each complexity metric.
omplexity metrics are divided in two groups: (a) process and (b) code metrics.
Process metrics refer to metrics that have been extracted from the project tracking system (i.e. fixing time, comments, reopening and severity).
Code metrics are directly computed using the source code used to fix a given bug (i.e. files impacted, changesets required, hunks and chunks).
We acknowledge that these complexity metrics only represent an abstraction of the actual complexity of a given bug as they cannot account for the actual thought processes and expertise required to craft a fix.
However, we believe that they are an accurate abstraction.
Moreover, they are used in several studies in the field also rely on these metrics to accurately approximate the complexity of bug \cite{WeiÃŸ2007,Saha2014,Nam2013,Anvik2006,Nagappan}.

\begin{figure*}
\centering
\includegraphics[page=1, width=0.45\textwidth]{extract/Rplots}
\includegraphics[page=2, width=0.45\textwidth]{extract/Rplots} \\
\includegraphics[page=3, width=0.45\textwidth]{extract/Rplots}
\includegraphics[page=4, width=0.45\textwidth]{extract/Rplots} \\
\includegraphics[page=5, width=0.45\textwidth]{extract/Rplots}
\includegraphics[page=6, width=0.45\textwidth]{extract/Rplots}

\label{fig:boxplots}}

\end{figure*}

\begin{figure*}
\centering
\includegraphics[page=7, width=0.45\textwidth]{extract/Rplots}
\includegraphics[page=8, width=0.45\textwidth]{extract/Rplots} \\
\includegraphics[page=9, width=0.45\textwidth]{extract/Rplots}

\caption{Complexity metrics boxplots. From left to right and top to bottom: Duplicate, Fixing time, Comments, Reopening, Files impacted, Severity, Changesets, Hunks and Chunks.
\label{fig:boxplots}}

\end{figure*}

\input{tabs/apache-types}
\input{tabs/netbeans-types}
\input{tabs/overall-types}


\\ \vspace{0.1cm} {\bf Duplicate: }
The duplicate metric represents the number of time a bug gets resolved using the {\it duplicate} label while referencing one of the {\it resolved/fixed} bug of our dataset.
The process metric is useful to approximate the impact of a given bug on the community.
Indeed, for a bug to be resolved using the {\it duplicate}, it means that the bug has been reported before.
The more a bug gets reported by the community, the more people are impacted enough to report it.
Note that, for a bug$_a$ to be resolved using the {\it duplicate} label and referencing bug$_b$, bug$_b$ does not have to resolved itself.
Indeed, bug$_b$ could be under investigation (i.e. {\it unconfirmed}) or being fixed (i.e. {\it new} or {\it assigned}).

In the Apache ecosystem, the types that are the most likely to get duplicated, ordered by ascending mean duplication rate, are T3 (0.016) $\textless$ T2 (0.022) $\textless$ T1 (0.026) $\textless$ T4 (0.029) and they represent 14.8\%, 8.1\%, 14.5\% and 62.6\% of the total duplications, respectively.
The differences between duplication means by types, however, are only significant in 33.33\% (4/12) of the case.
Indeed, the mean duplication are only significant on the following cases: T1 vs. T3, T3 vs. T4.
For the Apache ecosystem, we can conclude that $T4_{dup}^1 \gg T1_{dup}^2 \gg T3_{dup}^4$.
We use the notation  $x_{m}^r \gg y_{m}^r$ ($x_{m}^r \ll y_{m}^r$) to represent that $x$, along the metric $m$, is significantly greater (lower) than $y$, along the same metric, according to the mann-whitney tests ($\alpha \textless 0.05$).
$r$ represents the rank of $x$ ($y$) according to $m$ from 1 (higher percentage) to 4 (lower percentage).
In the netbeans ecosystem, we have a different order with T2 (0.067) \textless T3 (0.074) \textless T1 (0.086) \textless T4 (0.113) and they represent 0.6\%, 23.3\%, 2.5\% and 73.6\% of the overall duplication, respectively. Also, we have $T4_{dup}^1 \gg T3_{dup}^2$ for the netbeans ecosystem.

Overall, the complexity of bug types, duplication wise, is as follows:  $T4_{dup}^{1} \gg T1_{dup}^{3} > T3_{dup}^{2} \gg T2_{dup}^{4}$.


\\ \vspace{0.1cm} {\bf Fixing time: } The fixing time metric represents the time it took for the bug report to go from the {\it new} state to the {\it closed} state.
If the bug report is reopenned, then the time it took for the bug to go from the {\it assigned} state to the {\it closed} state is added to the first time.
A bug report can be reopnned several time and all the times are added.
In this section, the time is expressed in days.

In the Apache ecosystem, the types that take the most time to fix are  $T2_{time}^3
% (\mu:115.16, 17.4\%)
 \gg T1_{time}^2
% (\mu:91.5, 21.8\%)
 \gg T4_{time}^1
% (\mu:52.8, 62.6 \%)
 \gg T3_{time}^4
 % (\mu:35.9 13.5)
$.
The results for the Apache ecosystem might appear surprising at first sight.
Indeed, the types requiring the fewer fix location take the longer to fix.
However, this is concordant to the finding of Saha {\it et al.} on long lived bug \cite{Saha2014} where they discovered that the bugs that stay open the longest are, in fact, bugs that take the fewest location to fix.
In netbeans ecosystem, however, the order of bug type along the fixing time metric is different: $T4_{time}^1
% (\mu:128.83, 73\%)
 > T2_{time}^4
% (\mu:111.9, 0.9\%)
 \gg T1_{time}^3
% (\mu:92.76, 2.3\%)
 > T3_{time}^2
% (\mu:87.03, 23.8)
$.
This contradicts the finding of Saha {\it et al.}, however, they did not study the Netbeans ecosystem in their paper \cite{Saha2014}.
When combined, both ecosystem amounts to the following order
$
T2_{time}^4
% (\mu: 114.63, 4.4\%)
 >
T4_{time}^1
% (\mu:106.06, 67.6\%)
 \gg
T1_{time}^3
% (\mu:91.91, 6.5\%)
 \gg
T3_{time}^2
% (\mu: 73.21, 21.6\%)
$.

\\ \vspace{0.1cm} {\bf Comments: }
The comments metric counts how many comments hae been posted by the community on project tracking system.
This third process metric evaluate the complexity of a given bug in a sense that if it takes more comments (explanation) from the reporter or the assignee to craft a fix, then the bug must be more complex to understand.
The relationship between comments and complexity have been proven in previous studies \cite{Zhang2013,Zhang2012}.
It is also used for bug prediction approaches \cite{DAmbros2010,Bhattacharya2011}.

The analysis of the mann-whitney test matrix, in respect of comments, for the Apache ecosystem give us the following ordering of bug types:
$
T4_{comment}^1
% (\mu: 8.31, 68.3\%)
 \gg
T2_{comment}^4
% (\mu: 5.04, 7\%)
 \gg
T3_{comment}^2
% (\mu: 4.42, 15.2\%)
 >
T1_{comment}^3
% (\mu: 4.36, 9.5\%)
$.
In the Netbeans ecosystem, the bug types follows a different order:
$
T4_{comment}^1
% (\mu: 6.06, 70.4\%)
 \gg
T3_{comment}^2
% (\mu: 4.73, 26.5\%)
 >
T1_{comment}^3
% (\mu: 4.69, 2.4\%)
 \gg
T2_{comment}^4
% (\mu: 4.43, 0.7\%)
$.
When combined, our two ecosystems amounts to the following:
$
T4_{comment}^1
% (\mu: 6.73, 69.6\%)
 \gg
T2_{comment}^4
% (\mu: 4.94, 3.1\%)
 >
T3_{comment}^2
% (\mu: 4.64, 22.2\%)
 \gg
T1_{comment}^3
% (\mu: 4.45, 5.1\%)
$.

\\ \vspace{0.1cm} {\bf Reopening: }
The reopening metric counts how many time a given bug gets reopenned.
For a bug to be reopen, developer must craft a fix, submit it, the fix be accepted by the quality insurance team and the closed.
Then, the reporter tests the new version of the project, with the fix, and discover that the fix does not resolve the reported problem.
If a bug report is reopenned it means that the fix was arguably hard to come up with or the report was hard to understand.
Predicting which bug will be reopen is a research field per se
\cite{Zimmermann2012}\cite{Shihab2010}\cite{Lo2013}.
In the Apache and Netbeans ecosystems, the order to types, according the the reopening metric, is the same:
$
T4_{reop}^1
% (\mu: 0.07, 64.6\%)
 >
T2_{reop}^4
% (\mu: 0.07, 10.1\%)
 \gg
T3_{reop}^3
% (\mu: 0.03, 11.5\%)
 \gg
T1_{reop}^2
% (\mu: 0.06, 13.8\%)
$.
and
$
T4_{reop}^1 >
% (\mu: 0.09, 74.5\%)
T2_{reop}^4 >
% (\mu: 0.08, 0.9\%)
T3_{reop}^2 >
% (\mu: 0.06, 22.7\%)
T1_{reop}^3
% (\mu: 0.05, 1.9\%)
$, respectively.
When combined, however, the order does change:
$
T4_{reop}^1
% (\mu: 0.09, 71.7\%)
 >
T2_{reop}^4
% (\mu: 0.07, 3.5\%)
 >
T1_{reop}^3
% (\mu: 0.06, 5.3\%)
 \gg
T3_{reop}^2
% (\mu: 0.05, 19.5\%)
$.

\\ \vspace{0.1cm} {\bf Severity: } The severity metric reports the degree of impact of the report on the software.
Predicting the severity of a given report is an active research field
\cite{Menzies2008,Guo2010,Lamkanfi2010,Tian2012,ValdiviaGarcia2014, Havelund2015} and it helps to prioritization of fixes \cite{Xuan2012}.
The severity is a textual value (blocker, critical, major, normal, minor, trivial) and the mann-whitney test only accepts numerial input.
Consequently, we had to assign numerical values to each severity.
We chose to assign values from 1 to 6 for trivial, minor, normal, major, critical and blocker severities, respectively.
The bug type ordering, according to the severity metrics are
$
T4_{sev}^1
% (\mu: 3.84, 56\%)
 \gg
T3_{sev}^2
% (\mu: 3.64, 22.2\%)
 \gg
T2_{sev}^4
% (\mu: 3.50, 8.6\%)
 >
T1_{sev}^3
% (\mu: 3.42, 13.2\%)
$,
$
T2_{sev}^4
% (\mu: 4.36, 1\%)
>
T1_{sev}^3
% (\mu: 4.31, 3.1\%)
 \gg
T3_{sev}^2
% (\mu: 4.02, 31.4\%)
 \gg
T4_{sev}^1
% (\mu: 3.98, 64.5\%)
$
and
$
T4_{sev}^1
% (\mu: 3.94, 61.8\%)
 \gg
T3_{sev}^2
% (\mu: 3.92, 28.4\%)
 >
T1_{sev}^3
% (\mu: 3.68, 6.4\%)
 >
T2_{sev}^4
% (\mu: 3.64, 3.4\%)
$
for Apache, Netbeans and Overall ecosystems, respectively.


\\ \vspace{0.1cm} {\bf Files impacted: } The file impacted metrics measures how many files have been modified for the bug report to be closed.
Unsurprisingly, Types 4 and 2 are the ones with the most files impacted.
Indeed, according to their definitions, presented in Figure \ref{fig:bug-taxo}, Types 1 and 3 only need a modification in one location.
However, this metric is still interesting as we get to determine the location span of Types 2 and 4.
In Apache, types 4 structures are wider than types 2.
(
$
T4_{files}^1
% (\mu: 5.63, 79.9\%)
 \gg
T2_{files}^2
% (\mu: 4.38, 10.5\%)
 \gg
T3_{files}^3
% (\mu: 0.99, 5.9\%)
< = >
T1_{files}^4
% (\mu: 0.99, 3.7\%)
$)
while in Netbeans, types 2 are wider (
$
T2_{files}^3
% (\mu: 8.80, 1.3\%)
 \gg
T4_{files}^1
% (\mu: 8.40, 91\%)
 \gg
T3_{files}^2
% (\mu: 1.30, 6.8\%)
< = >
T1_{files}^4
% (\mu: 1.74, 0.8\%)
$).
Overall, types 4 impacts more files than types 2 while types 1 and 2 impacts only 1 file (
$
T4_{files}^1
% (\mu: 7.58, 91.9\%)
 \gg
T2_{files}^3
% (\mu: 5.10, 3.6\%)
 \gg
T3_{files}^2
% (\mu: 1.22, 6.6\%)
< = >
T1_{files}^4
% (\mu: 1.20, 1.5\%)
$).

\\ \vspace{0.1cm} {\bf Changesets: } The changeset metrics registers how many changesets (or commits/patch/fix) have been required to close the bug report.
In the project tracking system, changesets to resolve the bug are proposed and analysed by the community, automated quality insurance tools and the quality insurance team itself.
Each changeset can be either accepted and applied to the source code or dismissed.
The number of changesets (or versions of a given changeset) it takes before an integration can hint us about the complexity of the fix.
In case the bug report gets reopen and new changesets proposed, the new changesets (after the reopening) are added to the old ones (before the reopening).
For the Apache ecosystem, we found the following:
$
T4_{changesets}^1
% (\mu: 12.86, 89.7\%)
 \gg
T2_{changesets}^2
% (\mu: 4.68, 5.5\%)
 \gg
T1_{changesets}^4
% (\mu: 1, 1.9\%)
 <=>
T3_{changesets}^3
% (\mu: 1, 2.9\%)
$.
In the Netbeans ecosystem, the order stays the same at the exception of Types 1 and 2 that switch position from 3 to 2 and 2 to 3, respectively.
$
T4_{changesets}^1
% (\mu: 1.87, 76.4\%)
 \gg
T1_{changesets}^3
% (\mu: 1.09, 2\%)
 >
T2_{changesets}^4
% (\mu: 1.08, 0.6\%)
 >
T3_{changesets}^2
% (\mu: 1.07, 21\%)
$.
Overall, types 4 are the most complex bug in terms of changeset submitted (
$
T4_{changesets}^1
% (\mu: 5.16, 85.9\%)
 \gg
T2_{changesets}^3
% (\mu: 4.10, 4.1\%)
 \gg
T3_{changesets}^2
% (\mu: 1.05, 8.1\%)
 \gg
T1_{changesets}^4
% (\mu: 1.02, 1.9\%)
$).

While results have been published on bug-fix patterns \cite{Pan2008}, smell introduction\cite{Tufano2015, Eyolfson2011}, to the best of our knowledge, no one interested themselves in how many iterations of a patch were required to close a bug report besides us.

\\ \vspace{0.1cm} {\bf Hunks: } The hunks metric counts the number of consecutive code blocks of modified, added or deleted lines in textual files.
Hunks are used to determine, in each file, how many different places the developer has modified.
This metric is widely use for bug insertion prediction \cite{Kim2006,Jung2009, Rosen2015} and bug-fix comprehension \cite{Pan2008}.
Unsurprisingly, in our ecosystems, there is a relationship between the number of files modified and the hunks.
Indeed, the number of code blocks modified is likely to rise as with the number of modified files as the hunks metric will be at least 1 per file.
Consequently, we find that Types 2 and 4, that requires many files to get fixed, are the ones that have significantly higer scores for the hunks metric; Apache ecosystem:
$
T4_{hunks}^1
% (\mu: 2305.87, 96\%)
 \gg
T2_{hunks}^2
% (\mu: 561, 3.9\%)
 \gg
T3_{hunks}^3
% (\mu: 4.02, 0.1\%)
 \gg
T1_{hunks}^4
% (\mu: 3.8, 0\%)
$,
Netbeans ecosystem:
$
T4_{hunks}^1
% (\mu: 40.20, 93.1\%)
 \gg
T2_{hunks}^3
% (\mu: 21.89, 0.7\%)
 \gg
T3_{hunks}^2
% (\mu: 5.15, 5.8\%)
 \gg
T1_{hunks}^4
% (\mu: 4.41, 0.5\%)
$,
and overall
$
T4_{hunks}^1
% (\mu: 718.58, 95.8\%)
 \gg
T2_{hunks}^2
% (\mu: 474.88, 3.8\%)
 \gg
T1_{hunks}^4
% (\mu: 3.98, 0.1\%)
 \gg
T3_{hunks}^3
% (\mu: 4.85, 0.3\%)
$.

\\ \vspace{0.1cm} {\bf Churns: } The last metrics, churns, counts the number of lines modified.
The churn value for a line change should be at least two as the line has to be deleted first and then added back with the modifications.
Once again, this is a widely used metrics in the field \cite{Kim2006,Pan2008,Jung2009, Rosen2015}.
Once again, Types 4 and 2 are the ones with the most churns; Apache ecosystem
$
T4_{churns}^1
% (\mu: 27249.77, 91.9\%)
 \gg
T2_{churns}^2
% (\mu: 14184.87, 8\%)
 \gg
T1_{churns}^4
% (\mu: 18.76, 0\%)
 >
T3_{churns}^3
% (\mu: 16.95, 0\%)
$,
Netbeans ecosystem:
$
T4_{churns}^1
% (\mu: 61.89, 94\%)
 \gg
T2_{churns}^3
% (\mu: 32.26, 0.7\%)
 \gg
T3_{churns}^2
% (\mu: 6.73, 4.9\%)
 \gg
T1_{churns}^4
% (\mu: 5.09, 0.3\%)
$.
and overall :
$
T4_{churns}^1
% (\mu: 61.89, 94\%)
 \gg
T2_{churns}^2
% (\mu: 32.26, 0.7\%)
 \gg
T1_{churns}^4
% (\mu: 6.73, 4.9\%)
 \gg
T3_{churns}^3
% (\mu: 5.09, 0.3\%)
$.
\input{tabs/chisq}
\input{tabs/combined}

Assuming that each one of our nine complexity metrics bring an equal information about the complexity of a given bug, we scored each type with a simple system.
We counted how many times each bug type obtained each position in our nine rankings and multiply them by 4 for the first place, 3 for the second, 2 for the third and 1 for the fourth place.
We did the same simple analysis on the rank of each type for each metric, to take into account the frequence of bug types in our calculation, and multiply both values.
The complexity scores we calculated are as follows: 1330, 1750, 2580 and 7120 for bug types 1, 2, 3 and 4, respectively.
According to these complexity scores, types 3 and 4 are more complex than types 1 and 2.
In order to confirm or infirm the validity of our complexity scores, we ran our experiments again.
This time, we combined types 1 \& 2 and types 3 \& 4 for the two ecosystems.
As shown by Table \ref{tab:combined-one}, our complexity scores are meaningful.
Indeed, Types 3 \& 4 are statistically more complex ($\gg$) than Types 1 \& 2 according to the duplicate, fixing time, comments, files impacted, changesets, hunks and churns complexity metrics.
Also, Types 3 \& 4 get reopen more than  types 1 \& 2, in average, but the result of the mann-whitney test is not conclusive (i.e. $\alpha>0.05$).
Out of our nine complexity metrics, the only one where Types 1 \& 2 perform {\it worst} than Types 3 \& 4 is the severity.



% \input{tabs/combinedTwo}
